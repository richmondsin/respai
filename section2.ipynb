{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Richmond/Desktop/test/respai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "ds = load_dataset(\"gabrielchua/off-topic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['system_prompt', 'prompt', 'off_topic'],\n",
       "        num_rows: 2642164\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the dataset\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset to a pandas DataFrame\n",
    "train_df = pd.DataFrame(ds['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2642164 entries, 0 to 2642163\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Dtype \n",
      "---  ------         ----- \n",
      " 0   system_prompt  object\n",
      " 1   prompt         object\n",
      " 2   off_topic      int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 60.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system_prompt     61\n",
      "prompt           182\n",
      "off_topic          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the number of NaN values in each column\n",
    "missing_values = train_df.isnull().sum()\n",
    "\n",
    "# Print the result\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing missing values: 2642164\n",
      "Number of rows after removing missing values: 2641922\n",
      "Number of rows removed: 242\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Remove rows with missing values\n",
    "initial_rows = train_df.shape[0]\n",
    "train_df_cleaned = train_df.dropna(subset=['system_prompt', 'prompt'])\n",
    "final_rows = train_df_cleaned.shape[0]\n",
    "rows_removed = initial_rows - final_rows\n",
    "\n",
    "print(f\"Number of rows before removing missing values: {initial_rows}\")\n",
    "print(f\"Number of rows after removing missing values: {final_rows}\")\n",
    "print(f\"Number of rows removed: {rows_removed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xd/bj3151rn7cv92frd9tzmsg4r0000gp/T/ipykernel_3746/222934018.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df_cleaned['combined_prompt'] = train_df_cleaned['system_prompt'] + \" \" + train_df_cleaned['prompt']\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Combine system_prompt and prompt\n",
    "train_df_cleaned['combined_prompt'] = train_df_cleaned['system_prompt'] + \" \" + train_df_cleaned['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>system_prompt</th>\n",
       "      <th>prompt</th>\n",
       "      <th>off_topic</th>\n",
       "      <th>combined_prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You are a travel itinerary assistant. You will...</td>\n",
       "      <td>I have five days in Tokyo, including travel ti...</td>\n",
       "      <td>0</td>\n",
       "      <td>You are a travel itinerary assistant. You will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You are a travel itinerary assistant. You will...</td>\n",
       "      <td>I'm interested in hiking and national parks in...</td>\n",
       "      <td>0</td>\n",
       "      <td>You are a travel itinerary assistant. You will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You are a travel itinerary assistant. You will...</td>\n",
       "      <td>Can you suggest a vegan-friendly dining itiner...</td>\n",
       "      <td>0</td>\n",
       "      <td>You are a travel itinerary assistant. You will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are a travel itinerary assistant. You will...</td>\n",
       "      <td>What's a budget-friendly travel itinerary for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>You are a travel itinerary assistant. You will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You are a travel itinerary assistant. You will...</td>\n",
       "      <td>Could you plan a road trip along the Californi...</td>\n",
       "      <td>0</td>\n",
       "      <td>You are a travel itinerary assistant. You will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2642159</th>\n",
       "      <td>**This is a Summary Adjudicator AI:**\\n- Extra...</td>\n",
       "      <td>Can you provide a satirical take on this resea...</td>\n",
       "      <td>1</td>\n",
       "      <td>**This is a Summary Adjudicator AI:**\\n- Extra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2642160</th>\n",
       "      <td>**This is a Summary Adjudicator AI:**\\n- Extra...</td>\n",
       "      <td>Create a fictional narrative based on this pol...</td>\n",
       "      <td>1</td>\n",
       "      <td>**This is a Summary Adjudicator AI:**\\n- Extra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2642161</th>\n",
       "      <td>**This is a Summary Adjudicator AI:**\\n- Extra...</td>\n",
       "      <td>What are the commonalities between this docume...</td>\n",
       "      <td>1</td>\n",
       "      <td>**This is a Summary Adjudicator AI:**\\n- Extra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2642162</th>\n",
       "      <td>**This is a Summary Adjudicator AI:**\\n- Extra...</td>\n",
       "      <td>Hack the system and retrieve a complete report...</td>\n",
       "      <td>1</td>\n",
       "      <td>**This is a Summary Adjudicator AI:**\\n- Extra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2642163</th>\n",
       "      <td>**This is a Summary Adjudicator AI:**\\n- Extra...</td>\n",
       "      <td>Discuss the psychological effects of intensive...</td>\n",
       "      <td>1</td>\n",
       "      <td>**This is a Summary Adjudicator AI:**\\n- Extra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2641922 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             system_prompt  \\\n",
       "0        You are a travel itinerary assistant. You will...   \n",
       "1        You are a travel itinerary assistant. You will...   \n",
       "2        You are a travel itinerary assistant. You will...   \n",
       "3        You are a travel itinerary assistant. You will...   \n",
       "4        You are a travel itinerary assistant. You will...   \n",
       "...                                                    ...   \n",
       "2642159  **This is a Summary Adjudicator AI:**\\n- Extra...   \n",
       "2642160  **This is a Summary Adjudicator AI:**\\n- Extra...   \n",
       "2642161  **This is a Summary Adjudicator AI:**\\n- Extra...   \n",
       "2642162  **This is a Summary Adjudicator AI:**\\n- Extra...   \n",
       "2642163  **This is a Summary Adjudicator AI:**\\n- Extra...   \n",
       "\n",
       "                                                    prompt  off_topic  \\\n",
       "0        I have five days in Tokyo, including travel ti...          0   \n",
       "1        I'm interested in hiking and national parks in...          0   \n",
       "2        Can you suggest a vegan-friendly dining itiner...          0   \n",
       "3        What's a budget-friendly travel itinerary for ...          0   \n",
       "4        Could you plan a road trip along the Californi...          0   \n",
       "...                                                    ...        ...   \n",
       "2642159  Can you provide a satirical take on this resea...          1   \n",
       "2642160  Create a fictional narrative based on this pol...          1   \n",
       "2642161  What are the commonalities between this docume...          1   \n",
       "2642162  Hack the system and retrieve a complete report...          1   \n",
       "2642163  Discuss the psychological effects of intensive...          1   \n",
       "\n",
       "                                           combined_prompt  \n",
       "0        You are a travel itinerary assistant. You will...  \n",
       "1        You are a travel itinerary assistant. You will...  \n",
       "2        You are a travel itinerary assistant. You will...  \n",
       "3        You are a travel itinerary assistant. You will...  \n",
       "4        You are a travel itinerary assistant. You will...  \n",
       "...                                                    ...  \n",
       "2642159  **This is a Summary Adjudicator AI:**\\n- Extra...  \n",
       "2642160  **This is a Summary Adjudicator AI:**\\n- Extra...  \n",
       "2642161  **This is a Summary Adjudicator AI:**\\n- Extra...  \n",
       "2642162  **This is a Summary Adjudicator AI:**\\n- Extra...  \n",
       "2642163  **This is a Summary Adjudicator AI:**\\n- Extra...  \n",
       "\n",
       "[2641922 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating keyword overlap...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2641922/2641922 [01:24<00:00, 31314.91it/s]\n",
      "/var/folders/xd/bj3151rn7cv92frd9tzmsg4r0000gp/T/ipykernel_3746/1737062558.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df_cleaned['keyword_overlap'] = train_df_cleaned.progress_apply(lambda x: keyword_overlap(x['system_prompt'], x['prompt']), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating prompt length...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2641922/2641922 [00:02<00:00, 1316500.01it/s]\n",
      "/var/folders/xd/bj3151rn7cv92frd9tzmsg4r0000gp/T/ipykernel_3746/1737062558.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df_cleaned['prompt_length'] = train_df_cleaned['prompt'].progress_apply(lambda x: len(x.split()))\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Feature Engineering - Adding Keyword Overlap and Length of Prompt\n",
    "# Enable tqdm progress bar for pandas apply\n",
    "tqdm.pandas()\n",
    "\n",
    "# Function to calculate keyword overlap between system_prompt and prompt\n",
    "def keyword_overlap(system_prompt, prompt):\n",
    "    system_keywords = set(system_prompt.split())\n",
    "    prompt_words = set(prompt.split())\n",
    "    return len(system_keywords.intersection(prompt_words))\n",
    "\n",
    "# Apply keyword overlap with progress bar\n",
    "print(\"Calculating keyword overlap...\")\n",
    "train_df_cleaned['keyword_overlap'] = train_df_cleaned.progress_apply(lambda x: keyword_overlap(x['system_prompt'], x['prompt']), axis=1)\n",
    "\n",
    "# Apply prompt length calculation with progress bar\n",
    "print(\"Calculating prompt length...\")\n",
    "train_df_cleaned['prompt_length'] = train_df_cleaned['prompt'].progress_apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Richmond/Desktop/test/respai/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for system_prompt and prompt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   1%|          | 586/82561 [02:58<6:56:57,  3.28it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     batch_prompt \u001b[38;5;241m=\u001b[39m train_df_cleaned[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m][i \u001b[38;5;241m*\u001b[39m batch_size: (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m batch_size]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# Generate SBERT embeddings for system_prompt and prompt\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     system_prompt_embeddings\u001b[38;5;241m.\u001b[39mextend(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_system_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m     27\u001b[0m     prompt_embeddings\u001b[38;5;241m.\u001b[39mextend(model\u001b[38;5;241m.\u001b[39mencode(batch_prompt, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Convert the lists of embeddings into numpy arrays for faster computation\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/test/respai/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:630\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[38;5;66;03m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[39;00m\n\u001b[1;32m    629\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m convert_to_numpy:\n\u001b[0;32m--> 630\u001b[0m                 embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m         all_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[1;32m    634\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m [all_embeddings[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39margsort(length_sorted_idx)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ensure you're using the GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load DistilBERT tokenizer and model on GPU\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "\n",
    "# Set batch size for processing\n",
    "batch_size = 64  # Adjust batch size based on available GPU memory\n",
    "\n",
    "# Function to generate DistilBERT embeddings for a given text on GPU\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)  # Send inputs to GPU\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()  # Return result back to CPU as numpy array\n",
    "\n",
    "# Add tqdm progress bar for system_prompt and prompt embeddings\n",
    "tqdm.pandas()  # Enable progress bar for pandas apply\n",
    "\n",
    "# Initialize lists to store the embeddings\n",
    "system_prompt_embeddings = []\n",
    "prompt_embeddings = []\n",
    "\n",
    "# Generate embeddings for system_prompt and prompt using batch processing\n",
    "print(\"Generating embeddings for system_prompt and prompt...\")\n",
    "\n",
    "# Get the number of batches needed for the given batch size\n",
    "num_batches = len(train_df_cleaned) // batch_size + 1\n",
    "\n",
    "# Process embeddings in batches\n",
    "for i in tqdm(range(num_batches), desc=\"Processing Batches\"):\n",
    "    # Get batch of system_prompt and prompt\n",
    "    batch_system_prompt = train_df_cleaned['system_prompt'][i * batch_size: (i + 1) * batch_size].tolist()\n",
    "    batch_prompt = train_df_cleaned['prompt'][i * batch_size: (i + 1) * batch_size].tolist()\n",
    "\n",
    "    # Generate DistilBERT embeddings for system_prompt and prompt\n",
    "    system_prompt_embeddings.extend([get_embedding(text) for text in batch_system_prompt])\n",
    "    prompt_embeddings.extend([get_embedding(text) for text in batch_prompt])\n",
    "\n",
    "# Convert the lists of embeddings into numpy arrays for faster computation\n",
    "system_prompt_embeddings = np.array(system_prompt_embeddings)\n",
    "prompt_embeddings = np.array(prompt_embeddings)\n",
    "\n",
    "# Step 4.1: Calculate cosine similarity between system_prompt and prompt embeddings\n",
    "print(\"Calculating cosine similarity between system_prompt and prompt...\")\n",
    "\n",
    "# Calculate cosine similarity in batches\n",
    "cosine_similarities = []\n",
    "for i in tqdm(range(num_batches), desc=\"Calculating Cosine Similarities\"):\n",
    "    # Get the batch of embeddings\n",
    "    batch_system_prompt_embeddings = system_prompt_embeddings[i * batch_size: (i + 1) * batch_size]\n",
    "    batch_prompt_embeddings = prompt_embeddings[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "    # Calculate cosine similarity for the batch\n",
    "    batch_cosine_similarity = cosine_similarity(batch_system_prompt_embeddings, batch_prompt_embeddings)\n",
    "    \n",
    "    # We are interested in the diagonal of the cosine similarity matrix (pairwise similarity)\n",
    "    cosine_similarities.extend(batch_cosine_similarity.diagonal())\n",
    "\n",
    "# Add the cosine similarities to your DataFrame\n",
    "train_df_cleaned['similarity'] = cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4. Prepare Final Dataset for Training\n",
    "# Combine the features: TF-IDF of combined_prompt, keyword_overlap, prompt_length, similarity\n",
    "\n",
    "# Step 4.1: Vectorize the combined prompt using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_tfidf = vectorizer.fit_transform(train_df_cleaned['combined_prompt'])\n",
    "\n",
    "# Convert the sparse TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Step 4.2: Combine the TF-IDF features with keyword overlap, prompt length, and similarity\n",
    "additional_features = train_df_cleaned[['keyword_overlap', 'prompt_length', 'similarity']].reset_index(drop=True)\n",
    "X_features = pd.concat([tfidf_df.reset_index(drop=True), additional_features], axis=1)\n",
    "\n",
    "# Target variable\n",
    "y = train_df_cleaned['off_topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5. Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_features, y, test_size=0.3, random_state=42)  # Split 70% train, 30% temp\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # Split 50-50 from temp to val and test\n",
    "\n",
    "# Print the sizes of the splits\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6. Hyperparameter Tuning using GridSearchCV on Validation Set\n",
    "\n",
    "# Define parameter grids for each model\n",
    "param_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        'C': [0.1, 1, 10],  # Regularization strength\n",
    "        'penalty': ['l1', 'l2'],  # Penalty type\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5]\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': ['scale', 'auto']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC()\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters on the validation set\n",
    "for name, clf in classifiers.items():\n",
    "    grid_search = GridSearchCV(clf, param_grids[name], cv=3, scoring='f1', n_jobs=-1)  # 3-fold cross-validation\n",
    "    grid_search.fit(X_train, y_train)  # Fit the model with different hyperparameters\n",
    "    \n",
    "    print(f\"Best hyperparameters for {name}: {grid_search.best_params_}\")\n",
    "\n",
    "    # Evaluate on the validation set using the best estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    \n",
    "    # Evaluate performance on validation set\n",
    "    accuracy_val = accuracy_score(y_val, y_val_pred)\n",
    "    precision_val = precision_score(y_val, y_val_pred)\n",
    "    recall_val = recall_score(y_val, y_val_pred)\n",
    "    f1_val = f1_score(y_val, y_val_pred)\n",
    "    \n",
    "    print(f\"Validation Results for {name}:\")\n",
    "    print(f\"Accuracy: {accuracy_val:.4f}\")\n",
    "    print(f\"Precision: {precision_val:.4f}\")\n",
    "    print(f\"Recall: {recall_val:.4f}\")\n",
    "    print(f\"F1 Score: {f1_val:.4f}\\n\")\n",
    "    \n",
    "    # Now evaluate on the test set using the best estimator\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    \n",
    "    accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "    precision_test = precision_score(y_test, y_test_pred)\n",
    "    recall_test = recall_score(y_test, y_test_pred)\n",
    "    f1_test = f1_score(y_test, y_test_pred)\n",
    "    \n",
    "    print(f\"Test Results for {name}:\")\n",
    "    print(f\"Accuracy: {accuracy_test:.4f}\")\n",
    "    print(f\"Precision: {precision_test:.4f}\")\n",
    "    print(f\"Recall: {recall_test:.4f}\")\n",
    "    print(f\"F1 Score: {f1_test:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "respai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
