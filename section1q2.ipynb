{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Richmond/Desktop/test/respai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  \n",
    "import torch  \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score \n",
    "from inference import predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your SGHateCheck data from CSV files\n",
    "files = [  # List of file paths for the test case datasets\n",
    "    'sghatecheckdata/ms_testcases_all.csv',\n",
    "    'sghatecheckdata/ss_testcases_all.csv',\n",
    "    'sghatecheckdata/ta_testcases_all.csv',\n",
    "    'sghatecheckdata/zh_testcases_all.csv'\n",
    "]\n",
    "\n",
    "# Combine data from all CSV files into a single DataFrame\n",
    "all_data = pd.concat([pd.read_csv(file) for file in files])  # Read and concatenate all data into one DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map true labels from text to numerical values (0 and 1)\n",
    "label_mapping = {\n",
    "    'hateful': 1,  # Map 'hateful' to 1\n",
    "    'non-hateful': 0,  # Map 'non-hateful' to 0\n",
    "}\n",
    "\n",
    "# Drop rows where t_gold is NaN\n",
    "all_data = all_data.dropna(subset=['t_gold'])  \n",
    "\n",
    "# Convert true labels in the DataFrame to numerical format\n",
    "all_data['true_labels'] = all_data['t_gold'].map(label_mapping)  # Apply mapping to 't_gold' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the input data for predictions\n",
    "batch_text = all_data['c_testcase'].astype(str).tolist()  # Extract text from test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation on the combined dataset\n",
    "predicted_results = predict(batch_text)  # Call the prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions for all thresholds from the first category\n",
    "first_category = next(iter(predicted_results.values()))  # Get the first category's results\n",
    "predicted_high_recall = first_category['predictions']['high_recall']  # Extract high recall predictions\n",
    "predicted_balanced = first_category['predictions']['balanced']  # Extract balanced predictions\n",
    "predicted_high_precision = first_category['predictions']['high_precision']  # Extract high precision predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the predicted labels and true labels to tensors for evaluation\n",
    "true_labels_tensor = torch.tensor(all_data['true_labels'].tolist())  # Convert true labels column to a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate High Recall predictions\n",
    "predicted_high_recall_tensor = torch.tensor(predicted_high_recall)  # Convert high recall predictions to a tensor\n",
    "accuracy_high_recall = accuracy_score(true_labels_tensor.numpy(), predicted_high_recall_tensor.numpy())\n",
    "precision_high_recall = precision_score(true_labels_tensor.numpy(), predicted_high_recall_tensor.numpy())\n",
    "recall_high_recall = recall_score(true_labels_tensor.numpy(), predicted_high_recall_tensor.numpy())\n",
    "f1_high_recall = f1_score(true_labels_tensor.numpy(), predicted_high_recall_tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Balanced predictions\n",
    "predicted_balanced_tensor = torch.tensor(predicted_balanced)  # Convert balanced predictions to a tensor\n",
    "accuracy_balanced = accuracy_score(true_labels_tensor.numpy(), predicted_balanced_tensor.numpy())\n",
    "precision_balanced = precision_score(true_labels_tensor.numpy(), predicted_balanced_tensor.numpy())\n",
    "recall_balanced = recall_score(true_labels_tensor.numpy(), predicted_balanced_tensor.numpy())\n",
    "f1_balanced = f1_score(true_labels_tensor.numpy(), predicted_balanced_tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate High Precision predictions\n",
    "predicted_high_precision_tensor = torch.tensor(predicted_high_precision)  # Convert high precision predictions to a tensor\n",
    "accuracy_high_precision = accuracy_score(true_labels_tensor.numpy(), predicted_high_precision_tensor.numpy())\n",
    "precision_high_precision = precision_score(true_labels_tensor.numpy(), predicted_high_precision_tensor.numpy())\n",
    "recall_high_precision = recall_score(true_labels_tensor.numpy(), predicted_high_precision_tensor.numpy())\n",
    "f1_high_precision = f1_score(true_labels_tensor.numpy(), predicted_high_precision_tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High Recall Predictions:\n",
      "Accuracy: 0.5761\n",
      "Precision: 0.7633\n",
      "Recall: 0.5566\n",
      "F1 Score: 0.6438\n",
      "\n",
      "Balanced Predictions:\n",
      "Accuracy: 0.5121\n",
      "Precision: 0.7950\n",
      "Recall: 0.3922\n",
      "F1 Score: 0.5252\n",
      "\n",
      "High Precision Predictions:\n",
      "Accuracy: 0.4741\n",
      "Precision: 0.8189\n",
      "Recall: 0.3027\n",
      "F1 Score: 0.4420\n"
     ]
    }
   ],
   "source": [
    "# Print out the evaluation results for all thresholds\n",
    "print(\"High Recall Predictions:\")\n",
    "print(f\"Accuracy: {accuracy_high_recall:.4f}\")  \n",
    "print(f\"Precision: {precision_high_recall:.4f}\")  \n",
    "print(f\"Recall: {recall_high_recall:.4f}\")  \n",
    "print(f\"F1 Score: {f1_high_recall:.4f}\")  \n",
    "\n",
    "print(\"\\nBalanced Predictions:\")\n",
    "print(f\"Accuracy: {accuracy_balanced:.4f}\")  \n",
    "print(f\"Precision: {precision_balanced:.4f}\")  \n",
    "print(f\"Recall: {recall_balanced:.4f}\")  \n",
    "print(f\"F1 Score: {f1_balanced:.4f}\")  \n",
    "\n",
    "print(\"\\nHigh Precision Predictions:\")\n",
    "print(f\"Accuracy: {accuracy_high_precision:.4f}\")  \n",
    "print(f\"Precision: {precision_high_precision:.4f}\")  \n",
    "print(f\"Recall: {recall_high_precision:.4f}\")  \n",
    "print(f\"F1 Score: {f1_high_precision:.4f}\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "respai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
